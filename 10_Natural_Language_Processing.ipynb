{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dried-morning",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-cycle",
   "metadata": {},
   "source": [
    "## NLP - Part One - The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stylish-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "working-holder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bu\n"
     ]
    }
   ],
   "source": [
    "path=\"@Source\\\\06-NLP-and-Text-Data\\\\shakespeare.txt\"\n",
    "text=open(path, 'r').read()\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "altered-admission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted are\n",
      "  From his low tract and look another way:\n",
      "    So thou, thy self out-going in thy noon:\n",
      "    Unlooked on diest unless thou get a son.\n",
      "\n",
      "\n",
      "                     8\n",
      "  Music to hear, why hear'st thou music sadly?\n",
      "  Sweets with sweets war not, joy delights in joy:\n",
      "  Why lov'st thou that which t\n"
     ]
    }
   ],
   "source": [
    "print(text[4500:4800]) #spacing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "political-cartridge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward.\n",
      "  HELENA. Inspired merit so by breath is barr'd.\n",
      "    It is not so with Him that all things knows,\n",
      "    As 'tis with us that square our guess by shows;\n",
      "    But most it is presumption in us when\n",
      "    The help of heaven we count the act of men.\n",
      "    Dear sir, to my endeavours give consent;\n",
      "    Of heaven, not me, make an experiment.\n",
      "    I am not an impostor, that proclaim  \n",
      "    Myself against the level of mine aim;\n",
      "    But know I think, and think I know most sure,\n",
      "    My art is not past power n\n"
     ]
    }
   ],
   "source": [
    "print(text[140500:141000]) #HELENA is speaking - character name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "original-handy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '>',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " ']',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '|',\n",
       " '}']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab=sorted(set(text)) #unique characters\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "conventional-flower",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-marker",
   "metadata": {},
   "source": [
    "## NLP - Part Two - Text Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "inner-virtue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '\\n')\n",
      "(1, ' ')\n",
      "(2, '!')\n",
      "(3, '\"')\n",
      "(4, '&')\n",
      "(5, \"'\")\n",
      "(6, '(')\n",
      "(7, ')')\n",
      "(8, ',')\n",
      "(9, '-')\n",
      "(10, '.')\n",
      "(11, '0')\n",
      "(12, '1')\n",
      "(13, '2')\n",
      "(14, '3')\n",
      "(15, '4')\n",
      "(16, '5')\n",
      "(17, '6')\n",
      "(18, '7')\n",
      "(19, '8')\n",
      "(20, '9')\n",
      "(21, ':')\n",
      "(22, ';')\n",
      "(23, '<')\n",
      "(24, '>')\n",
      "(25, '?')\n",
      "(26, 'A')\n",
      "(27, 'B')\n",
      "(28, 'C')\n",
      "(29, 'D')\n",
      "(30, 'E')\n",
      "(31, 'F')\n",
      "(32, 'G')\n",
      "(33, 'H')\n",
      "(34, 'I')\n",
      "(35, 'J')\n",
      "(36, 'K')\n",
      "(37, 'L')\n",
      "(38, 'M')\n",
      "(39, 'N')\n",
      "(40, 'O')\n",
      "(41, 'P')\n",
      "(42, 'Q')\n",
      "(43, 'R')\n",
      "(44, 'S')\n",
      "(45, 'T')\n",
      "(46, 'U')\n",
      "(47, 'V')\n",
      "(48, 'W')\n",
      "(49, 'X')\n",
      "(50, 'Y')\n",
      "(51, 'Z')\n",
      "(52, '[')\n",
      "(53, ']')\n",
      "(54, '_')\n",
      "(55, '`')\n",
      "(56, 'a')\n",
      "(57, 'b')\n",
      "(58, 'c')\n",
      "(59, 'd')\n",
      "(60, 'e')\n",
      "(61, 'f')\n",
      "(62, 'g')\n",
      "(63, 'h')\n",
      "(64, 'i')\n",
      "(65, 'j')\n",
      "(66, 'k')\n",
      "(67, 'l')\n",
      "(68, 'm')\n",
      "(69, 'n')\n",
      "(70, 'o')\n",
      "(71, 'p')\n",
      "(72, 'q')\n",
      "(73, 'r')\n",
      "(74, 's')\n",
      "(75, 't')\n",
      "(76, 'u')\n",
      "(77, 'v')\n",
      "(78, 'w')\n",
      "(79, 'x')\n",
      "(80, 'y')\n",
      "(81, 'z')\n",
      "(82, '|')\n",
      "(83, '}')\n"
     ]
    }
   ],
   "source": [
    "for pair in enumerate(vocab):\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "norman-region",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '\"': 3,\n",
       " '&': 4,\n",
       " \"'\": 5,\n",
       " '(': 6,\n",
       " ')': 7,\n",
       " ',': 8,\n",
       " '-': 9,\n",
       " '.': 10,\n",
       " '0': 11,\n",
       " '1': 12,\n",
       " '2': 13,\n",
       " '3': 14,\n",
       " '4': 15,\n",
       " '5': 16,\n",
       " '6': 17,\n",
       " '7': 18,\n",
       " '8': 19,\n",
       " '9': 20,\n",
       " ':': 21,\n",
       " ';': 22,\n",
       " '<': 23,\n",
       " '>': 24,\n",
       " '?': 25,\n",
       " 'A': 26,\n",
       " 'B': 27,\n",
       " 'C': 28,\n",
       " 'D': 29,\n",
       " 'E': 30,\n",
       " 'F': 31,\n",
       " 'G': 32,\n",
       " 'H': 33,\n",
       " 'I': 34,\n",
       " 'J': 35,\n",
       " 'K': 36,\n",
       " 'L': 37,\n",
       " 'M': 38,\n",
       " 'N': 39,\n",
       " 'O': 40,\n",
       " 'P': 41,\n",
       " 'Q': 42,\n",
       " 'R': 43,\n",
       " 'S': 44,\n",
       " 'T': 45,\n",
       " 'U': 46,\n",
       " 'V': 47,\n",
       " 'W': 48,\n",
       " 'X': 49,\n",
       " 'Y': 50,\n",
       " 'Z': 51,\n",
       " '[': 52,\n",
       " ']': 53,\n",
       " '_': 54,\n",
       " '`': 55,\n",
       " 'a': 56,\n",
       " 'b': 57,\n",
       " 'c': 58,\n",
       " 'd': 59,\n",
       " 'e': 60,\n",
       " 'f': 61,\n",
       " 'g': 62,\n",
       " 'h': 63,\n",
       " 'i': 64,\n",
       " 'j': 65,\n",
       " 'k': 66,\n",
       " 'l': 67,\n",
       " 'm': 68,\n",
       " 'n': 69,\n",
       " 'o': 70,\n",
       " 'p': 71,\n",
       " 'q': 72,\n",
       " 'r': 73,\n",
       " 's': 74,\n",
       " 't': 75,\n",
       " 'u': 76,\n",
       " 'v': 77,\n",
       " 'w': 78,\n",
       " 'x': 79,\n",
       " 'y': 80,\n",
       " 'z': 81,\n",
       " '|': 82,\n",
       " '}': 83}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ind={char: ind for ind, char in enumerate(vocab)}\n",
    "char_to_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "straight-turkey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ind['H']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "colored-happening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', ',', '-', '.', '0', '1',\n",
       "       '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '>', '?',\n",
       "       'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
       "       'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
       "       '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i',\n",
       "       'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v',\n",
       "       'w', 'x', 'y', 'z', '|', '}'], dtype='<U1')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_to_char=np.array(vocab)\n",
    "ind_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "unexpected-harvest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_to_char[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "twenty-steel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5445609,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_test=np.array([char_to_ind[c] for c in text])\n",
    "encoded_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "statistical-flood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1, 12,  0,  1,  1, 31, 73, 70, 68,  1, 61, 56, 64,\n",
       "       73, 60, 74, 75,  1, 58, 73, 60, 56, 75, 76, 73, 60, 74,  1, 78, 60,\n",
       "        1, 59, 60, 74, 64, 73, 60,  1, 64, 69, 58, 73, 60, 56, 74, 60,  8,\n",
       "        0,  1,  1, 45, 63, 56, 75,  1, 75, 63, 60, 73, 60, 57, 80,  1, 57,\n",
       "       60, 56, 76, 75, 80,  5, 74,  1, 73, 70, 74, 60,  1, 68, 64])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_test[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "running-greensboro",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose mi\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-console",
   "metadata": {},
   "source": [
    "## NLP - Part Three - Creating Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "robust-engineer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bu\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "respiratory-partnership",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line=\"From fairest creatures we desire increase\"\n",
    "len(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "secret-moldova",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rhymes - so we need 3 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "entertaining-lucas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines='''\n",
    "From fairest creatures we desire increase,\n",
    "  That thereby beauty's rose might never die,\n",
    "  But as the riper should by time decease,\n",
    "'''\n",
    "\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "broad-grass",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45005"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len=120\n",
    "\n",
    "total_num_seq=len(text)//(120+1) # // - classic division/true division; 120+1 because 0 indexing\n",
    "total_num_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "crucial-adrian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.TensorSliceDataset"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_dataset=tf.data.Dataset.from_tensor_slices(encoded_test)\n",
    "\n",
    "type(char_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "experimental-shannon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "12\n",
      "0\n",
      "1\n",
      "1\n",
      "31\n",
      "73\n",
      "70\n",
      "68\n",
      "1\n",
      "61\n",
      "56\n",
      "64\n",
      "73\n",
      "60\n",
      "74\n",
      "75\n",
      "1\n",
      "58\n",
      "73\n",
      "60\n",
      "56\n",
      "75\n",
      "76\n",
      "73\n",
      "60\n",
      "74\n",
      "1\n",
      "78\n",
      "60\n",
      "1\n",
      "59\n",
      "60\n",
      "74\n",
      "64\n",
      "73\n",
      "60\n",
      "1\n",
      "64\n",
      "69\n",
      "58\n",
      "73\n",
      "60\n",
      "56\n",
      "74\n",
      "60\n",
      "8\n",
      "0\n",
      "1\n",
      "1\n",
      "45\n",
      "63\n",
      "56\n",
      "75\n",
      "1\n",
      "75\n",
      "63\n",
      "60\n",
      "73\n",
      "60\n",
      "57\n",
      "80\n",
      "1\n",
      "57\n",
      "60\n",
      "56\n",
      "76\n",
      "75\n",
      "80\n",
      "5\n",
      "74\n",
      "1\n",
      "73\n",
      "70\n",
      "74\n",
      "60\n",
      "1\n",
      "68\n",
      "64\n",
      "62\n",
      "63\n",
      "75\n",
      "1\n",
      "69\n",
      "60\n",
      "77\n",
      "60\n",
      "73\n",
      "1\n",
      "59\n",
      "64\n",
      "60\n",
      "8\n",
      "0\n",
      "1\n",
      "1\n",
      "27\n",
      "76\n",
      "75\n",
      "1\n",
      "56\n",
      "74\n",
      "1\n",
      "75\n",
      "63\n",
      "60\n",
      "1\n",
      "73\n",
      "64\n",
      "71\n",
      "60\n",
      "73\n",
      "1\n",
      "74\n",
      "63\n",
      "70\n",
      "76\n",
      "67\n",
      "59\n",
      "1\n",
      "57\n",
      "80\n",
      "1\n",
      "75\n",
      "64\n",
      "68\n",
      "60\n",
      "1\n",
      "59\n",
      "60\n",
      "58\n",
      "60\n",
      "56\n",
      "74\n",
      "60\n",
      "8\n",
      "0\n",
      "1\n",
      "1\n",
      "33\n",
      "64\n",
      "74\n",
      "1\n",
      "75\n",
      "60\n",
      "69\n",
      "59\n",
      "60\n",
      "73\n",
      "1\n",
      "63\n",
      "60\n",
      "64\n",
      "73\n",
      "1\n",
      "68\n",
      "64\n",
      "62\n",
      "63\n",
      "75\n",
      "1\n",
      "57\n",
      "60\n",
      "56\n",
      "73\n",
      "1\n",
      "63\n",
      "64\n",
      "74\n",
      "1\n",
      "68\n",
      "60\n",
      "68\n",
      "70\n",
      "73\n",
      "80\n",
      "21\n",
      "0\n",
      "1\n",
      "1\n",
      "27\n",
      "76\n",
      "75\n",
      "1\n",
      "75\n",
      "63\n",
      "70\n",
      "76\n",
      "1\n",
      "58\n",
      "70\n",
      "69\n",
      "75\n",
      "73\n",
      "56\n",
      "58\n",
      "75\n",
      "60\n",
      "59\n",
      "1\n",
      "75\n",
      "70\n",
      "1\n",
      "75\n",
      "63\n",
      "64\n",
      "69\n",
      "60\n",
      "1\n",
      "70\n",
      "78\n",
      "69\n",
      "1\n",
      "57\n",
      "73\n",
      "64\n",
      "62\n",
      "63\n",
      "75\n",
      "1\n",
      "60\n",
      "80\n",
      "60\n",
      "74\n",
      "8\n",
      "0\n",
      "1\n",
      "1\n",
      "31\n",
      "60\n",
      "60\n",
      "59\n",
      "5\n",
      "74\n",
      "75\n",
      "1\n",
      "75\n",
      "63\n",
      "80\n",
      "1\n",
      "67\n",
      "64\n",
      "62\n",
      "63\n",
      "75\n",
      "5\n",
      "74\n",
      "1\n",
      "61\n",
      "67\n",
      "56\n",
      "68\n",
      "60\n",
      "1\n",
      "78\n",
      "64\n",
      "75\n",
      "63\n",
      "1\n",
      "74\n",
      "60\n",
      "67\n",
      "61\n",
      "9\n",
      "74\n",
      "76\n",
      "57\n",
      "74\n",
      "75\n",
      "56\n",
      "69\n",
      "75\n",
      "64\n",
      "56\n",
      "67\n",
      "1\n",
      "61\n",
      "76\n",
      "60\n",
      "67\n",
      "8\n",
      "0\n",
      "1\n",
      "1\n",
      "38\n",
      "56\n",
      "66\n",
      "64\n",
      "69\n",
      "62\n",
      "1\n",
      "56\n",
      "1\n",
      "61\n",
      "56\n",
      "68\n",
      "64\n",
      "69\n",
      "60\n",
      "1\n",
      "78\n",
      "63\n",
      "60\n",
      "73\n",
      "60\n",
      "1\n",
      "56\n",
      "57\n",
      "76\n",
      "69\n",
      "59\n",
      "56\n",
      "69\n",
      "58\n",
      "60\n",
      "1\n",
      "67\n",
      "64\n",
      "60\n",
      "74\n",
      "8\n",
      "0\n",
      "1\n",
      "1\n",
      "45\n",
      "63\n",
      "80\n",
      "1\n",
      "74\n",
      "60\n",
      "67\n",
      "61\n",
      "1\n",
      "75\n",
      "63\n",
      "80\n",
      "1\n",
      "61\n",
      "70\n",
      "60\n",
      "8\n",
      "1\n",
      "75\n",
      "70\n",
      "1\n",
      "75\n",
      "63\n",
      "80\n",
      "1\n",
      "74\n",
      "78\n",
      "60\n",
      "60\n",
      "75\n",
      "1\n",
      "74\n",
      "60\n",
      "67\n",
      "61\n",
      "1\n",
      "75\n",
      "70\n",
      "70\n",
      "1\n",
      "58\n",
      "73\n",
      "76\n",
      "60\n",
      "67\n",
      "21\n",
      "0\n",
      "1\n",
      "1\n",
      "45\n",
      "63\n",
      "70\n",
      "76\n",
      "1\n",
      "75\n",
      "63\n",
      "56\n",
      "75\n",
      "1\n",
      "56\n",
      "73\n",
      "75\n",
      "1\n",
      "69\n",
      "70\n",
      "78\n",
      "1\n",
      "75\n",
      "63\n",
      "60\n",
      "1\n",
      "78\n",
      "70\n",
      "73\n",
      "67\n",
      "59\n",
      "5\n",
      "74\n",
      "1\n",
      "61\n",
      "73\n",
      "60\n",
      "74\n",
      "63\n",
      "1\n",
      "70\n",
      "73\n",
      "69\n",
      "56\n",
      "68\n",
      "60\n",
      "69\n",
      "75\n",
      "8\n",
      "0\n",
      "1\n",
      "1\n",
      "26\n",
      "69\n",
      "59\n",
      "1\n",
      "70\n",
      "69\n",
      "67\n",
      "80\n",
      "1\n",
      "63\n",
      "60\n",
      "73\n",
      "56\n",
      "67\n",
      "59\n",
      "1\n",
      "75\n",
      "70\n",
      "1\n",
      "75\n",
      "63\n",
      "60\n",
      "1\n",
      "62\n",
      "56\n",
      "76\n",
      "59\n",
      "80\n",
      "1\n",
      "74\n",
      "71\n",
      "73\n",
      "64\n",
      "69\n",
      "62\n",
      "8\n",
      "0\n",
      "1\n",
      "1\n",
      "48\n",
      "64\n",
      "75\n",
      "63\n",
      "64\n",
      "69\n",
      "1\n",
      "75\n",
      "63\n",
      "64\n",
      "69\n",
      "60\n",
      "1\n",
      "70\n",
      "78\n",
      "69\n",
      "1\n",
      "57\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "for item in char_dataset.take(500):\n",
    "    print(item.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "communist-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqences=char_dataset.batch(seq_len+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fossil-terminology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq_targets(seq):\n",
    "    input_txt=seq[:-1] #Hello my nam\n",
    "    target_txt=seq[1:] #ello my name\n",
    "    return input_txt, target_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "honest-coach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((120,), (120,)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=seqences.map(create_seq_targets)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "divine-chambers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0\n",
      "  1  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74\n",
      "  1 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45\n",
      " 63 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74\n",
      " 60  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75]\n",
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But\n",
      "\n",
      "\n",
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0  1\n",
      "  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74  1\n",
      " 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45 63\n",
      " 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74 60\n",
      "  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75  1]\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But \n"
     ]
    }
   ],
   "source": [
    "for input_text, target_text in dataset.take(1):\n",
    "    print(input_text.numpy())\n",
    "    print(\"\".join(ind_to_char[input_text.numpy()]))\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    print(target_text.numpy())\n",
    "    print(\"\".join(ind_to_char[target_text.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "subjective-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "\n",
    "\n",
    "buffer_size=1000\n",
    "dataset=dataset.shuffle(buffer_size=buffer_size).batch(batch_size=batch_size,  drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "chubby-yesterday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((128, 120), (128, 120)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-judgment",
   "metadata": {},
   "source": [
    "## NLP - Part Four - Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "medieval-passage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size=len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "perceived-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "embed_dim=64 # power of 2, the sabe range of vocab_size\n",
    "rnn_neurons=1024\n",
    "\n",
    "def sparse_cat_loss(y_true, y_pred):\n",
    "    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "handmade-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "\n",
    "\n",
    "def create_model(vocab_size, embeded_dim, rnn_neurons, batch_size):\n",
    "    model=Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_dim, batch_input_shape=[batch_size, None]))\n",
    "    model.add(GRU(rnn_neurons, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))\n",
    "    model.add(Dense(vocab_size))\n",
    "    \n",
    "    model.compile('adam', loss=sparse_cat_loss) # function without ()!\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "accompanied-muslim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (128, None, 64)           5376      \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (128, None, 1024)         3348480   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (128, None, 84)           86100     \n",
      "=================================================================\n",
      "Total params: 3,439,956\n",
      "Trainable params: 3,439,956\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=create_model(vocab_size=vocab_size, \n",
    "                  embeded_dim=embed_dim, \n",
    "                  rnn_neurons=rnn_neurons,\n",
    "                  batch_size=batch_size)\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-anatomy",
   "metadata": {},
   "source": [
    "## NLP - Part Five - Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-norman",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
